# utils.py
from transformers import pipeline
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import numpy as np
import pandas as pd
import requests
import re

sentiment_model = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")
embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

def analyze_text(text: str, lang: str = 'en') -> dict:
    # Î‘Î½ Î¸ÎµÏ‚, Î¼Ï€Î¿ÏÎµÎ¯Ï‚ Î½Î± Ï€ÏÎ¿ÏƒÎ±ÏÎ¼ÏŒÏƒÎµÎ¹Ï‚ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î³Î¹Î± Î³Î»ÏŽÏƒÏƒÎ± Î® Î½Î± Ï„Î¿ Î±Î³Î½Î¿Î®ÏƒÎµÎ¹Ï‚ Ï€ÏÎ¿Ï‚ Ï„Î¿ Ï€Î±ÏÏŒÎ½
    sentiment = sentiment_model(text)[0]
    embedding = embedding_model.encode([text])[0]
    emoji_map = {
        "1 star": "ðŸ˜¡ Î Î¿Î»Ï Î±ÏÎ½Î·Ï„Î¹ÎºÏŒ",
        "2 stars": "ðŸ™ Î‘ÏÎ½Î·Ï„Î¹ÎºÏŒ",
        "3 stars": "ðŸ˜ ÎŸÏ…Î´Î­Ï„ÎµÏÎ¿",
        "4 stars": "ðŸ™‚ Î˜ÎµÏ„Î¹ÎºÏŒ",
        "5 stars": "ðŸ¤© Î Î¿Î»Ï Î¸ÎµÏ„Î¹ÎºÏŒ"
    }
    emoji = emoji_map.get(sentiment["label"], "ðŸ§ Î†Î³Î½Ï‰ÏƒÏ„Î¿")

    emoji_result = f"### {emoji} (score: {sentiment['score']:.2f})"
    return {
        "emoji_result": emoji_result,
        "details": {
            "Î£Ï…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î±": sentiment["label"],
            "Embedding Vector (Ï€ÏÏŽÏ„Î± 5)": embedding[:5].tolist(),
            "ÎœÎ®ÎºÎ¿Ï‚ Embedding": len(embedding)
        }
    }

def extract_keywords(text: str, lang: str = 'en') -> list:
    stop_words = "english" if lang == "en" else None
    tfidf = TfidfVectorizer(stop_words=stop_words, max_features=10)
    tfidf_matrix = tfidf.fit_transform([text])
    words = tfidf.get_feature_names_out()
    scores = tfidf_matrix.toarray()[0]
    sorted_keywords = sorted(zip(words, scores), key=lambda x: x[1], reverse=True)
    return [w for w, s in sorted_keywords]


def fetch_reddit_posts(subreddit: str, limit: int = 10) -> list:
    url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit={limit}"
    headers = {"User-agent": "TextLabBot 1.0"}
    try:
        response = requests.get(url, headers=headers)
        data = response.json()
        posts = [post['data']['title'] for post in data['data']['children'] if 'title' in post['data']]
        return posts
    except Exception as e:
        return []
