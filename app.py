# app.py
import streamlit as st
import pandas as pd
from utils import analyze_text, compare_texts, cluster_texts, extract_keywords, fetch_reddit_posts

st.set_page_config(page_title="TextLab", layout="wide")
st.title("ğŸ” TextLab - Î‘Î½Î¬Î»Ï…ÏƒÎ· ÎšÎµÎ¹Î¼Î­Î½Î¿Ï… & Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î¦Ï…ÏƒÎ¹ÎºÎ®Ï‚ Î“Î»ÏÏƒÏƒÎ±Ï‚")

lang = st.radio("Î•Ï€Î­Î»ÎµÎ¾Îµ Î³Î»ÏÏƒÏƒÎ± ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…:", options=["ğŸ‡¬ğŸ‡· Î•Î»Î»Î·Î½Î¹ÎºÎ¬", "ğŸ‡¬ğŸ‡§ Î‘Î³Î³Î»Î¹ÎºÎ¬"])

tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "ğŸ“„ Î‘Î½Î¬Î»Ï…ÏƒÎ· ÎšÎµÎ¹Î¼Î­Î½Î¿Ï…", "ğŸ†š Î£ÏÎ³ÎºÏÎ¹ÏƒÎ·", "ğŸ”— Clustering",
    "ğŸ§  Î›Î­Î¾ÎµÎ¹Ï‚-ÎšÎ»ÎµÎ¹Î´Î¹Î¬", "ğŸ“‚ CSV Î‘Î½Î¬Î»Ï…ÏƒÎ·", "ğŸ” Reddit Î•Î¾Î±Î³Ï‰Î³Î®"
])

with tab1:
    text_input = st.text_area("Î•Î¹ÏƒÎ®Î³Î±Î³Îµ ÎºÎµÎ¯Î¼ÎµÎ½Î¿:", height=200)
    if st.button("Î‘Î½Î¬Î»Ï…ÏƒÎ· ÎšÎµÎ¹Î¼Î­Î½Î¿Ï…"):
        if text_input.strip():
            with st.spinner("ğŸ” Î‘Î½Î±Î»ÏÎµÏ„Î±Î¹..."):
                result = analyze_text(text_input, lang=lang)
                st.subheader("ğŸ“Œ Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±:")
                st.markdown(result["emoji_result"])
                st.write(result["details"])
        else:
            st.warning("Î Î±ÏÎ±ÎºÎ±Î»Ï ÎµÎ¹ÏƒÎ¬Î³ÎµÏ„Îµ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Ï€ÏÏÏ„Î±.")

with tab2:
    col1, col2 = st.columns(2)
    with col1:
        text1 = st.text_area("ÎšÎµÎ¯Î¼ÎµÎ½Î¿ 1")
    with col2:
        text2 = st.text_area("ÎšÎµÎ¯Î¼ÎµÎ½Î¿ 2")
    if st.button("Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· ÎšÎµÎ¹Î¼Î­Î½Ï‰Î½"):
        if text1 and text2:
            score = compare_texts(text1, text2)
            st.metric("Cosine Similarity", f"{score:.2f}")
        else:
            st.warning("Î•Î¹ÏƒÎ®Î³Î±Î³Îµ ÎºÎ±Î¹ Ï„Î± Î´ÏÎ¿ ÎºÎµÎ¯Î¼ÎµÎ½Î±.")

with tab3:
    multi_input = st.text_area("Î•Î¹ÏƒÎ®Î³Î±Î³Îµ Ï€Î¿Î»Î»Î±Ï€Î»Î¬ ÎºÎµÎ¯Î¼ÎµÎ½Î± (Î­Î½Î± Î±Î½Î¬ Î³ÏÎ±Î¼Î¼Î®):", height=200)
    if st.button("Clustering"):
        texts = [t.strip() for t in multi_input.split("\n") if t.strip()]
        if len(texts) >= 2:
            clusters = cluster_texts(texts)
            for i, group in clusters.items():
                st.markdown(f"**Cluster {i+1}:**")
                st.write(group)
        else:
            st.warning("Î§ÏÎµÎ¹Î¬Î¶Î¿Î½Ï„Î±Î¹ Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Î½ 2 ÎºÎµÎ¯Î¼ÎµÎ½Î±.")

with tab4:
    tfidf_input = st.text_area("ÎšÎµÎ¯Î¼ÎµÎ½Î¿ Î³Î¹Î± ÎµÎ¾Î±Î³Ï‰Î³Î® Î»Î­Î¾ÎµÏ‰Î½-ÎºÎ»ÎµÎ¹Î´Î¹Î¬:", height=200)
    if st.button("Î›Î­Î¾ÎµÎ¹Ï‚-ÎšÎ»ÎµÎ¹Î´Î¹Î¬"):
        if tfidf_input.strip():
            keywords = extract_keywords(tfidf_input, lang)
            st.markdown("**Top Î»Î­Î¾ÎµÎ¹Ï‚:**")
            st.write(keywords)
        else:
            st.warning("Î•Î¹ÏƒÎ®Î³Î±Î³Îµ ÎºÎ¬Ï€Î¿Î¹Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿.")

with tab5:
    uploaded_file = st.file_uploader("ğŸ“‚ Î‘Î½Î­Î²Î±ÏƒÎµ CSV Î±ÏÏ‡ÎµÎ¯Î¿", type="csv")
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        col = st.selectbox("Î•Ï€Î­Î»ÎµÎ¾Îµ ÏƒÏ„Î®Î»Î· Î¼Îµ ÎºÎµÎ¯Î¼ÎµÎ½Î±:", df.columns)
        if st.button("Î‘Î½Î¬Î»Ï…ÏƒÎ· CSV"):
            with st.spinner("Î‘Î½Î¬Î»Ï…ÏƒÎ·..."):
                for idx, row in df[col].dropna().items():
                    st.markdown(f"### ğŸ”¹ ÎšÎµÎ¯Î¼ÎµÎ½Î¿ {idx + 1}")
                    res = analyze_text(row, lang)
                    st.markdown(res["emoji_result"])

with tab6:
    subreddit = st.text_input("ğŸ”— Î¥Ï€Î¿Ï†ÏŒÏÎ¿Ï…Î¼ (Ï€.Ï‡. greeklanguage, datascience)", value="datascience")
    limit = st.slider("Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ posts", 1, 50, 10)
    if st.button("Î¦Î­ÏÎµ Î±Î½Î±ÏÏ„Î®ÏƒÎµÎ¹Ï‚ Î±Ï€ÏŒ Reddit"):
        with st.spinner("Î¦ÏŒÏÏ„Ï‰ÏƒÎ·..."):
            posts = fetch_reddit_posts(subreddit, limit)
            if posts:
                for i, post in enumerate(posts):
                    st.markdown(f"### ğŸ§µ Post {i+1}")
                    res = analyze_text(post, lang)
                    st.markdown(f"ğŸ“Œ {post}")
                    st.markdown(res["emoji_result"])
            else:
                st.warning("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±.")
